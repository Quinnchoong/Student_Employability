# -*- coding: utf-8 -*-
"""Employability_Prediction_CP2_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18uFCEAozRq39AphqkC4cmfETopAvmi0w
"""

#import libraries
import pandas as pd
import numpy as np
import seaborn as sns
import missingno as msno

# Import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC # Import SVC here
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import GridSearchCV

#set max row and col for pandas dataframe
pd.set_option ('display.max_columns', None)
pd.set_option ('display.max_row',200)

#supress warnings
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount("/content/gdrive")

# Verify this path exists in your Google Drive
file_path = "/content/gdrive/MyDrive/Employability Prediction CP2 2025/Student_Employability_dataset_2025.xlsx"

# Import data
data = pd.read_excel("/content/gdrive/My Drive/Employability Prediction CP2 2025/Student_Employability_dataset_2025.xlsx")

print(data.head(10))

# === Exploring the dataset ===

# Check Dataset's number of rows and columns
data.shape

# Examine the Type of variables in the dataset
data. info ()

# Check on the dataset head
data.head(10)

# Descriptive Statics of dataset
numeric_data = data.select_dtypes(include='number')

# Create a comprehensive summary
summary_stats = pd.DataFrame({
    'Count': numeric_data.count(),
    'Mean': numeric_data.mean(),
    'Median': numeric_data.median(),
    'Mode': numeric_data.mode().iloc[0],  # Take the first mode value
    'Std Dev': numeric_data.std(),
    'Min': numeric_data.min(),
    '25%': numeric_data.quantile(0.25),
    '50%': numeric_data.quantile(0.50),  # Optional, same as Median
    '75%': numeric_data.quantile(0.75),
    'Max': numeric_data.max(),
    'Skewness': numeric_data.skew()
})

summary_stats = summary_stats
summary_stats

# === Univariate Analysis ===
# Copy the dataset and select only numeric columns
rf1 = data.select_dtypes(include=['float64', 'int64'])

# Set seaborn style
sns.set_palette("muted", color_codes=True)

# Loop through numeric columns
for col in rf1.columns:
    plt.figure(figsize=(12, 6))

    # Plot histogram with KDE
    sns.histplot(data=rf1, x=col, kde=True)

    # Compute descriptive stats
    col_min = rf1[col].min()
    col_max = rf1[col].max()
    col_mean = rf1[col].mean()
    col_median = rf1[col].median()
    col_mode = rf1[col].mode().iloc[0]
    col_skewness = rf1[col].skew()
    col_std = rf1[col].std()

    # Labels and title
    plt.xlabel(col)
    plt.ylabel("Count")
    plt.title(f"Univariate Analysis of {col}\n"
              f"min={col_min:.2f}, max={col_max:.2f}, "
              f"mean={col_mean:.2f}, median={col_median:.2f}, "
              f"mode={col_mode:.2f}, skewness={col_skewness:.2f}, std={col_std:.2f}")

    plt.show()

#Bivariate Analysis
from scipy.stats import chi2_contingency # Import the chi2_contingency function

# Prepare numeric data
data_numeric = data.select_dtypes(include='number')
dependent_var = 'STUDENT_PERFORMANCE_RATING'

# Cross-tabulations and Chi-square test
crosstab_list = []
chi2_data = []
boxplot_vars = []

for col in data_numeric.columns:
    if col != dependent_var:
        ctab = pd.crosstab(data_numeric[col], data_numeric[dependent_var])
        crosstab_list.append((col, ctab))

        # Chi-square test
        chi2_stat, p_val, dof, _ = chi2_contingency(ctab)
        chi2_data.append({
            "Variable": col,
            "Chi-Square": chi2_stat,
            "p-value": p_val,
            "Degrees of Freedom": dof
        })

        # Boxplot list
        boxplot_vars.append(col)

# Convert chi-square results to DataFrame
chi2_df_full = pd.DataFrame(chi2_data).sort_values("p-value")

# Display chi-square results using pandas' display function instead of ace_tools
# This provides a default way to display the DataFrame
display(chi2_df_full)

# Generate boxplots
for var in boxplot_vars:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=data_numeric[dependent_var], y=data_numeric[var], palette='pastel')
    plt.title(f'Boxplot of {var} by {dependent_var}')
    plt.xlabel(dependent_var)
    plt.ylabel(var)
    plt.tight_layout()
    plt.show()

# === Bivariate Analysis ===

# Correlation Heatmap
plt.figure(figsize=(12, 8))
# Select numeric columns from the DataFrame 'data'
numeric_columns = data.select_dtypes(include=['number'])
corr = numeric_columns.corr() # Calculate correlation using the selected numeric columns
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

#Data Preprocessing

# Check dulicates
data.duplicated().sum()

#Check for nulls
data.isnull().sum()

#Identify Zero Variance
zero_var_cols = [col for col in data.columns if data[col].nunique() <= 1]

print("Zero variance columns:", zero_var_cols)

# Detect outliers using IQR and manual bounds on Likert scale
outlier_summary = {}

# Select numeric columns from the original DataFrame 'data'
data_numeric = data.select_dtypes(include=['float64', 'int64'])

for col in data_numeric.columns:
   # IQR Method
    Q1 = data_numeric[col].quantile(0.25)
    Q3 = data_numeric[col].quantile(0.75)
    IQR = Q3 - Q1
    iqr_outliers = data_numeric[(data_numeric[col] < Q1 - 1.5 * IQR) | (data_numeric[col] > Q3 + 1.5 * IQR)]

    # Manual bound check (for Likert-type scales assumed from 1 to 5)
    manual_outliers = data_numeric[(data_numeric[col] < 1) | (data_numeric[col] > 5)]

    outlier_summary[col] = {
        "IQR Outlier Count": len(iqr_outliers),
        "Manual Bound Violation Count": len(manual_outliers)
    }

# Convert summary to DataFrame
outlier_summary_data = pd.DataFrame(outlier_summary).T
print(outlier_summary_data)

# Drop irrelevant columns
data.drop(columns=["Name_of_Student"], errors='ignore', inplace=True)

# Drop missing values
data.dropna(inplace=True)

# Encode target
data['CLASS'] = data['CLASS'].map({'Employable': 1, 'LessEmployable': 0})

# Check available styles
print(mpl.style.available)

# Replace 'seaborn-deep' with a valid style from the output above.
try:
    mpl.style.use('seaborn-v0_8-deep')
except OSError:
    # Fallback to a general seaborn style if the specific one is not available
    try:
        mpl.style.use('seaborn-v0_8')
    except OSError:
         # Fallback to a simpler style if seaborn styles are not working
         print("Warning: Seaborn styles not found, using 'ggplot' style.")
         mpl.style.use('ggplot')


# Count number of samples in each class
counts = data['CLASS'].value_counts().tolist()
labels = ['Employable' if val == 1 else 'Less Employable' for val in data['CLASS'].value_counts().index]

# Print values for verification
print(f"This is the headers: {labels}\nThis is the count: {counts}")

# Plot the bar chart
plt.figure(figsize=(6,4))
plt.bar(labels, counts, color=['steelblue', 'salmon'])
plt.title("Population Distribution by Employability Class", fontsize=14)
plt.xlabel("Employability Category")
plt.ylabel("Number of Students")
plt.tight_layout()
plt.show()

import math
#Gets the piechart vaariables
def pie_chart_var(col_name, data):
    counts = [g.shape[0] for _,g in  data.groupby(col_name)]
    headers = [g.loc[:,col_name].iloc[0] for _,g in  data.groupby(col_name)]

    #get percentages
    percentages = np.array(counts) / np.array(counts).sum() *100
    headers = [f"{label} - {round(percentage,2)}%" for label, percentage in zip(headers, percentages)]

    return counts, headers


#Creating the figure
col = 3
row = math.ceil(len(data.columns)/col)
fig,ax = plt.subplots(row,col, figsize=(20,10), layout='constrained')

for idx, ax in enumerate(ax.flatten()):
    if idx < len(data.columns):
        col_name = list(data.columns)[idx]
        counts, headers = pie_chart_var(col_name, data)
        ax.pie(counts, labels=headers)
        ax.set_title(f"Population {col_name.title()}")
    else:
        ax.remove()

fig.suptitle("Population Pie Chart by Features", fontsize=20)
plt.show()

# Correlation

print(f'This is the Skew\n{data.skew()}\n\nThis is the Kurtosis\n{data.kurtosis()}')

# Compute Spearman correlation matrix
spearman_corr = data.corr(method='spearman')

# Visualize Spearman correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(spearman_corr, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title("Spearman Rank Correlation Matrix")
plt.show()

# Identify features highly correlated with CLASS
correlation_with_target = spearman_corr['CLASS'].drop('CLASS').sort_values(ascending=False)
correlation_with_target

correlation_by_three = []

for col in (spearman_corr.columns):
    # Since Highest Correlation is always itself, it is first in the list
    correlation = spearman_corr[col].sort_values(ascending=False).index[0:4]
    correlation_by_three.append(correlation)

for cor in correlation_by_three:
    print(f"{cor[0]} is correlated to the student's: {list(cor[1:])}\n")

plot = sns.pairplot(data, kind='reg', hue='CLASS', height=1.8)
plot.savefig("pairplot.png", dpi=100)

# Normalize numeric features
num_cols = data.select_dtypes(include=['float64', 'int64']).columns
from sklearn.preprocessing import MinMaxScaler # Import MinMaxScaler
scaler = MinMaxScaler()
data[num_cols] = scaler.fit_transform(data[num_cols])

# Split features and target
X = data.drop(['CLASS'], axis=1)
y = data['CLASS']

print(X.columns.tolist())

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Scale the Feature
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_res)
X_test_scaled = scaler.transform(X_test)

# Output the shape of processed data
print("Train shape:", X_train_scaled.shape)
print("Test shape:", X_test_scaled.shape)

# Initialize models and Hyperparameter grids
model_defs = {
    'SVM': (SVC(probability=True, random_state=42), {
        'C': [0.1, 1, 10],
        'gamma': [0.1, 1, 10],
        'kernel': ['rbf']
    }),
    'Decision Tree': (DecisionTreeClassifier(random_state=42), {
        'max_depth': [5, 10, 15],
        'min_samples_split': [2, 5, 10]
    }),
    'Logistic Regression': (LogisticRegression(max_iter=1000, random_state=42), {
        'C': [0.1, 1, 10],
        'penalty': ['l2']
    }),
    'KNN': (KNeighborsClassifier(), {
        'n_neighbors': [3, 5, 7],
        'weights': ['uniform', 'distance']
    })
}

results = {}
best_model = None
best_f1 = 0

#Import necessary metrics and joblib
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import joblib

# Loop through and run grid search
for name, (model, params) in model_defs.items():
    print(f"Training {name}...")
    grid = GridSearchCV(model, params, cv=5, scoring='f1', n_jobs=-1)
    grid.fit(X_train_scaled, y_train_res)

    best_est = grid.best_estimator_
    y_pred = best_est.predict(X_test_scaled)
    y_prob = best_est.predict_proba(X_test_scaled)[:, 1]

    results[name] = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'roc_auc': roc_auc_score(y_test, y_prob),
        'confusion_matrix': confusion_matrix(y_test, y_pred),
        'best_params': grid.best_params_
    }

    if results[name]['f1'] > best_f1:
        best_f1 = results[name]['f1']
        best_model = best_est

# Save best model
joblib.dump(best_model, 'employability_predictor.pkl')

# Generate results table
results_df = pd.DataFrame(results).T
results_df.to_csv('model_performance.csv')
print(results_df)

# Confusion matrix plots
plt.figure(figsize=(12, 8))
for i, (name, res) in enumerate(results.items(), 1):
    plt.subplot(2, 2, i)
    sns.heatmap(res['confusion_matrix'], annot=True, fmt='d', cmap='Blues',
                xticklabels=['Less Employable', 'Employable'],
                yticklabels=['Less Employable', 'Employable'])
    plt.title(f"{name} (F1 = {res['f1']:.3f})")
plt.tight_layout()
plt.savefig('confusion_matrices.png')
plt.show()

# Plot Learning Curve for SVM (Best model)
from sklearn.model_selection import learning_curve # Import necessary modules
from sklearn.svm import SVC # Import SVC to recreate the best model

def plot_learning_curve(estimator, title, X, y):
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=5, scoring='accuracy', n_jobs=-1,
        train_sizes=np.linspace(.1, 1.0, 5), random_state=42)

    train_scores_mean = np.mean(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)

    plt.figure()
    plt.title(title)
    plt.xlabel("Training examples")
    plt.ylabel("Accuracy")
    plt.grid()
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")
    plt.legend(loc="best")
    plt.show()

# Use the best SVM model found during the grid search for the learning curve
# Access the best parameters for SVM from the 'results' dictionary and create a new SVC instance
best_svm_params = results["SVM"]["best_params"]
best_svm_model = SVC(probability=True, random_state=42, **best_svm_params)


# Use y_train_res which was the variable name used in the SMOTE step
plot_learning_curve(best_svm_model, "Learning Curve: SVM", X_train_scaled, y_train_res)

# Validation Curve with gamma=5
from sklearn.model_selection import validation_curve # Import validation_curve

param_range = [0.01, 0.1, 1, 10, 100]
train_scores, test_scores = validation_curve(
    SVC(kernel='rbf', gamma=5), X_train_res, y_train_res,
    param_name='C', param_range=param_range,
    cv=5, scoring='f1', n_jobs=-1
)

train_scores_mean = np.mean(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)

# Plot
plt.figure()
plt.semilogx(param_range, train_scores_mean, label="Training F1 Score", marker='o')
plt.semilogx(param_range, test_scores_mean, label="Cross-validation F1 Score", marker='s')
plt.title("Validation Curve (SVM with gamma=5)")
plt.xlabel("C Value (log scale)")
plt.ylabel("F1 Score")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import joblib

# Save the trained model
joblib.dump(best_model, 'employability_predictor.pkl')

# Save the scaler too (if you used one)
joblib.dump(scaler, 'scaler.pkl')

# Export out for Streamlit App
from google.colab import files
files.download('employability_predictor.pkl')
files.download('scaler.pkl')

print(X_train_res.columns.tolist())
print(X_train_res.dtypes)

# On test set
y_pred = best_model.predict(X_test_scaled)
pd.Series(y_pred).value_counts()

print("On Test Data:")
y_pred = best_model.predict(X_test_scaled)
print(pd.Series(y_pred).value_counts())

print("Confusion Matrix:")
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, y_pred))